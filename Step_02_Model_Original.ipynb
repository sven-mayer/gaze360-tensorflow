{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IMAGES = \"./data/imgs/\"\n",
    "PATH_MODELS = \"./models/\"\n",
    "\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "if not(os.path.isdir(PATH_MODELS)):\n",
    "    os.mkdir(PATH_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n",
      "2.0.0\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "#import tensorflow_lattice as tfl\n",
    "print(tf.__version__)\n",
    "print(tf.keras.backend.image_data_format())\n",
    "import scipy.io \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def pinball_loss(y_true, y_pred, tau=.5):\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this, loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    ```python\n",
    "    loss = pinball_loss([0., 0., 1., 1.], [1., 1., 1., 0.], tau=.1)\n",
    "    # loss = max(0.1 * (y_true - y_pred), (0.1 - 1) * (y_true - y_pred))\n",
    "    #      = (0.9 + 0.9 + 0 + 0.1) / 4\n",
    "    print('Loss: ', loss.numpy())  # Loss: 0.475\n",
    "    ```\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # broadcast the pinball slope along the batch dimension, and clip to\n",
    "    # acceptable values\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    delta_y = y_true - y_pred\n",
    "    pinball = tf.math.maximum(tau * delta_y, (tau - one) * delta_y)\n",
    "    return tf.reduce_mean(tf.keras.backend.batch_flatten(pinball), axis=-1)\n",
    "\n",
    "\n",
    "class PinballLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression, this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    ```python\n",
    "    pinball = tfa.losses.PinballLoss(tau=.1)\n",
    "    loss = pinball([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "    # loss = max(0.1 * (y_true - y_pred), (0.1 - 1) * (y_true - y_pred))\n",
    "    #      = (0.9 + 0.9 + 0 + 0.1) / 4\n",
    "    print('Loss: ', loss.numpy())  # Loss: 0.475\n",
    "    ```\n",
    "    Usage with the `compile` API:\n",
    "    ```python\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile('sgd', loss=tfa.losses.PinballLoss(tau=.1))\n",
    "    ```\n",
    "    Args:\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "        loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "        option will be determined by the usage context. For almost all cases\n",
    "        this defaults to `SUM_OVER_BATCH_SIZE`.\n",
    "        When used with `tf.distribute.Strategy`, outside of built-in training\n",
    "        loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
    "        `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
    "        https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
    "        for more details on this.\n",
    "      name: Optional name for the op.\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tau=.5,\n",
    "                 reduction=tf.keras.losses.Reduction.AUTO,\n",
    "                 name='pinball_loss'):\n",
    "        super(PinballLoss, self).__init__(reduction=reduction, name=name)\n",
    "        self.tau = tau\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return pinball_loss(y_true, y_pred, self.tau)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'tau': self.tau,\n",
    "        }\n",
    "        base_config = super(PinballLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./data/metadata_small.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train     126928\n",
       "unused     27653\n",
       "test       25969\n",
       "val        17038\n",
       "Name: Split, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random test image\n",
    "if False:\n",
    "    fig, ax = plt.subplots(3,5, figsize=(18,8))\n",
    "    for i in range (3):\n",
    "        for j in range (5):\n",
    "            path = df[(df.PersonIdentity == 0) & (df.Recording == 1)].sample().iloc[0].Path\n",
    "            img = Image.open(path)\n",
    "            ax[i][j].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df):\n",
    "    fileNames = df.Path.to_list()\n",
    "    gazeDirs = df.Gaze.to_list()\n",
    "    images = []\n",
    "    for fileName, gazeDirs in zip(fileNames, gazeDirs):\n",
    "        frame_number = int(fileName.split('/')[-1][:-4])\n",
    "        lists_sources = []\n",
    "        for j in range(-3,4):\n",
    "            name_frame = '/'.join(fileName.split('/')[:-1]+['%0.6d.jpg'%(frame_number+j)])\n",
    "            lists_sources.append(name_frame)\n",
    "            \n",
    "        item = (lists_sources,gazeDirs)\n",
    "        images.append(item)\n",
    "    return images\n",
    "\n",
    "def loader(path):\n",
    "    try:\n",
    "        #image = tf.io.read_file(path)\n",
    "        #image = tf.image.decode_jpeg(image)\n",
    "        #image = tf.image.resize(image, [224,224])\n",
    "        image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224,224))\n",
    "        return image\n",
    "    except Exception:\n",
    "        print(\"Error:\", path)\n",
    "        return np.zeros((224,224,3))\n",
    "        \n",
    "class GazeDataset(tf.data.Dataset):\n",
    "    def _generator(split, num_samples):\n",
    "        df = pd.read_pickle(\"./data/metadata_small.pkl\")\n",
    "        df = df[df.Split == split.decode()]\n",
    "        data = make_dataset(df)\n",
    "        while True:\n",
    "            random.shuffle(data)\n",
    "            for idx in range(0, len(data)-num_samples, num_samples):\n",
    "                lstImg = []\n",
    "                lstGaze = []\n",
    "                for i in range(num_samples):\n",
    "                    path_source, gaze = data[idx+i]\n",
    "                    # Reading data (line, record) from the file\n",
    "                    imgs = []\n",
    "                    for i, frame_path in enumerate(path_source):\n",
    "                        img = loader(frame_path)\n",
    "                        imgs.append(img)\n",
    "                    lstGaze.append(gaze)\n",
    "                    lstImg.append(np.stack(imgs))  \n",
    "                    #print(tf.shape(imgs))\n",
    "\n",
    "                yield np.stack(lstImg), np.stack(lstGaze)\n",
    "                #yield np.array(lstImg), np.array(lstGaze)\n",
    "    \n",
    "    def __new__(cls, split, num_samples):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.float32, tf.float32),\n",
    "            #=tf.float32,\n",
    "            output_shapes=([None, 7, 224,224,3], [None, 2]),\n",
    "            args=(split, num_samples,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 17:19:50.579977 0 0\n",
      "2020-04-04 17:19:53.726773 1 0\n",
      "Execution time: 6.657734099775553\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        i = 0\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            print(datetime.now(), epoch_num, i)\n",
    "            i = i + 1\n",
    "            break\n",
    "    tf.print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "    \n",
    "benchmark(GazeDataset(\"train\", BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 17:19:57.198906 x i\n"
     ]
    }
   ],
   "source": [
    "for sample in GazeDataset(\"train\", BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE):\n",
    "    # Performing a training step\n",
    "    print(datetime.now(), \"x\", \"i\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60, 7, 224, 224, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_Yaw(y_pred, y_true):\n",
    "    error = tf.math.abs(y_true[:,0] - y_pred[:,0])\n",
    "    error = 180.0 * tf.math.reduce_mean(error) / math.pi\n",
    "    return error\n",
    "\n",
    "def error_Pitch(y_pred, y_true):\n",
    "    error = tf.math.abs(y_true[:,1] - y_pred[:,1])\n",
    "    error = 180.0 * tf.math.reduce_mean(error) / math.pi\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 7, 1000)           3538984   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 7, 256)            256256    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 7, 256)            394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 4,584,234\n",
      "Trainable params: 1,045,250\n",
      "Non-trainable params: 3,538,984\n",
      "_________________________________________________________________\n",
      "Train for 2115 steps, validate for 283 steps\n",
      "Epoch 1/80\n",
      "2115/2115 [==============================] - 2253s 1s/step - loss: 0.2309 - error_Pitch: 11.5551 - error_Yaw: 41.3638 - val_loss: 0.3199 - val_error_Pitch: 14.6491 - val_error_Yaw: 58.6593\n",
      "Epoch 2/80\n",
      "2115/2115 [==============================] - 1969s 931ms/step - loss: 0.2088 - error_Pitch: 10.9764 - error_Yaw: 36.8868 - val_loss: 0.3330 - val_error_Pitch: 15.2246 - val_error_Yaw: 61.0950\n",
      "Epoch 3/80\n",
      "2115/2115 [==============================] - 1924s 910ms/step - loss: 0.1989 - error_Pitch: 10.8402 - error_Yaw: 34.7430 - val_loss: 0.3278 - val_error_Pitch: 15.2438 - val_error_Yaw: 59.8743\n",
      "Epoch 4/80\n",
      "2115/2115 [==============================] - 1975s 934ms/step - loss: 0.1834 - error_Pitch: 10.6046 - error_Yaw: 31.4286 - val_loss: 0.5087 - val_error_Pitch: 14.9005 - val_error_Yaw: 101.6851\n",
      "Epoch 6/80\n",
      "2115/2115 [==============================] - 1964s 929ms/step - loss: 0.1788 - error_Pitch: 10.5707 - error_Yaw: 30.4084 - val_loss: 0.4689 - val_error_Pitch: 16.1116 - val_error_Yaw: 91.3542\n",
      "Epoch 7/80\n",
      "2115/2115 [==============================] - 1903s 900ms/step - loss: 0.1734 - error_Pitch: 10.4636 - error_Yaw: 29.2880 - val_loss: 0.4992 - val_error_Pitch: 15.0716 - val_error_Yaw: 99.3442\n",
      "Epoch 8/80\n",
      "2115/2115 [==============================] - 1834s 867ms/step - loss: 0.1694 - error_Pitch: 10.3783 - error_Yaw: 28.4400 - val_loss: 0.5571 - val_error_Pitch: 15.6749 - val_error_Yaw: 111.9919\n",
      "Epoch 9/80\n",
      " 293/2115 [===>..........................] - ETA: 9:22 - loss: 0.1682 - error_Pitch: 10.3439 - error_Yaw: 28.2012"
     ]
    }
   ],
   "source": [
    "time = datetime.now()\n",
    "timeStr = str(time).split(\".\")[0].replace(\" \", \"-\").replace(\":\", \"-\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    resnet = tf.keras.applications.MobileNetV2()\n",
    "    resnet.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input((7, 224, 224, 3)),\n",
    "        tf.keras.layers.TimeDistributed(resnet),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256)),\n",
    "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 128, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 128, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 128)),\n",
    "        tf.keras.layers.Dense(2)\n",
    "    ])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss=PinballLoss(), metrics=[error_Pitch, error_Yaw])\n",
    "\n",
    "\n",
    "\n",
    "    path = \"%scrf.%s-gaze360{epoch:03d}-{val_loss:.4f}.hdf5.\" % (PATH_MODELS, time)\n",
    "    callbackSaver = tf.keras.callbacks.ModelCheckpoint(path, monitor='val_loss')\n",
    "    #model.fit_generator will be deprecated use fit instead\n",
    "    model.fit(GazeDataset(\"train\", BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "                        steps_per_epoch=int(len(df[df.Split == \"train\"])/BATCH_SIZE),\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=GazeDataset(\"val\", BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE),\n",
    "                        validation_steps=int(len(df[df.Split == \"val\"])/BATCH_SIZE),\n",
    "                        callbacks=[])\n",
    "                       #workers=30,\n",
    "                       #use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
